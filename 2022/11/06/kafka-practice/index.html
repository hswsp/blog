<!DOCTYPE html><html lang="en" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/profile.jpeg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Starry"><meta name="keywords" content=""><meta name="description" content="kafka-practice"><meta property="og:type" content="article"><meta property="og:title" content="Kafka两年踩过的一些非比寻常的坑"><meta property="og:url" content="https://www.blog.spphoto.top/2022/11/06/kafka-practice/index.html"><meta property="og:site_name" content="champagne&#39;s vicissitude"><meta property="og:description" content="kafka-practice"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://pbs.twimg.com/media/DTK99PsVwAAHVs3.jpg"><meta property="article:published_time" content="2022-11-06T04:33:56.000Z"><meta property="article:modified_time" content="2022-11-06T05:01:29.586Z"><meta property="article:author" content="Champagne"><meta property="article:tag" content="MQ"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://pbs.twimg.com/media/DTK99PsVwAAHVs3.jpg"><title>Kafka两年踩过的一些非比寻常的坑 - champagne&#39;s vicissitude</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_3619942_p0ons582io9.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/toubudaziji.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/daziyanse.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/gundongtiao.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/shubiao.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"www.blog.spphoto.top",root:"/",version:"1.9.2",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.2.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Starry</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> Home</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> Archives</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> Categories</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> Tags</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> About</a></li><li class="nav-item"><a class="nav-link" href="/Repositories/"><i class="iconfont icon-github-fill"></i> Repositories</a></li><li class="nav-item"><a class="nav-link" href="/gallery/"><i class="iconfont icon-image"></i> Galleries</a></li><li class="nav-item"><a class="nav-link" href="/links/"><i class="iconfont icon-link-fill"></i> Links</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(https://th.bing.com/th/id/R.9e589c0f4722f880f6d3389dc4fe4825?rik=wDqk1SdilPcP7g&riu=http%3a%2f%2fcloudfront.zekkei-japan.jp%2fimages%2farticles%2f4836255222117da4d3730be8a5f5dea3.jpg&ehk=V6RNjkmx3L%2frPFr0ILw%2fRmnD3pqhEQsQzc1XZ8uLJLo%3d&risl=&pid=ImgRaw&r=0) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="Kafka两年踩过的一些非比寻常的坑"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-11-06 12:33" pubdate>November 6, 2022 pm</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 7.1k words </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 60 mins</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">Kafka两年踩过的一些非比寻常的坑</h1><div class="markdown-body"><blockquote><p>本文章摘录自 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/YPkE3Tsu3RVbhfVZCBt1pQ">苏三说技术</a>。汲取前人Kafaka使用经验！</p></blockquote><h1 id="前言">前言</h1><p>我的上家公司是做餐饮系统的，每天中午和晚上用餐高峰期，系统的并发量不容小觑。为了保险起见，公司规定各部门都要在吃饭的时间轮流值班，防止出现线上问题时能够及时处理。</p><p>我当时在后厨显示系统团队，该系统属于订单的下游业务。用户点完菜下单后，订单系统会通过发<code>kafka</code>消息给我们系统，系统读取消息后，做业务逻辑处理，持久化订单和菜品数据，然后展示到划菜客户端。这样厨师就知道哪个订单要做哪些菜，有些菜做好了，就可以通过该系统出菜。系统自动通知服务员上菜，如果服务员上完菜，修改菜品上菜状态，用户就知道哪些菜已经上了，哪些还没有上。这个系统可以大大提高后厨到用户的效率。</p><figure><img src="https://cdn.jsdelivr.net/gh/hswsp/IMAGE_HOST/img/640.png" srcset="/img/loading.gif" lazyload alt="Image"><figcaption aria-hidden="true">Image</figcaption></figure><p>事实证明，这一切的关键是消息中间件：<code>kafka</code>，如果它有问题，将会直接影响到后厨显示系统的功能。</p><p>接下来，我跟大家一起聊聊使用<code>kafka</code>两年时间踩过哪些坑？</p><h1 id="顺序问题">顺序问题</h1><h2 id="为什么要保证消息的顺序">1. 为什么要保证消息的顺序？</h2><p>刚开始我们系统的商户很少，为了快速实现功能，我们没想太多。既然是走消息中间件<code>kafka</code>通信，订单系统发消息时将订单详细数据放在消息体，我们后厨显示系统只要订阅<code>topic</code>，就能获取相关消息数据，然后处理自己的业务即可。</p><p>不过这套方案有个关键因素：<strong>要保证消息的顺序</strong>。</p><p>为什么呢？</p><p>订单有很多状态，比如：下单、支付、完成、撤销等，不可能<code>下单</code>的消息都没读取到，就先读取<code>支付</code>或<code>撤销</code>的消息吧，如果真的这样，数据不是会产生错乱？</p><p>好吧，看来保证消息顺序是有必要的。</p><h2 id="如何保证消息顺序">2.如何保证消息顺序？</h2><p>我们都知道<code>kafka</code>的<code>topic</code>是无序的，但是一个<code>topic</code>包含多个<code>partition</code>，<strong>每个<code>partition</code>内部是有序的</strong>。</p><figure><img src="https://cdn.jsdelivr.net/gh/hswsp/IMAGE_HOST/img/641.png" srcset="/img/loading.gif" lazyload alt="Image"><figcaption aria-hidden="true">Image</figcaption></figure><p>如此一来，思路就变得清晰了：只要保证生产者写消息时，<strong>按照一定的规则写到同一个<code>partition</code></strong>，<strong>不同的消费者读不同的<code>partition</code>的消息</strong>，就能保证生产和消费者消息的顺序。</p><p>我们刚开始就是这么做的，同一个<code>商户编号</code>的消息写到同一个<code>partition</code>，<code>topic</code>中创建了<code>4</code>个<code>partition</code>，然后部署了<code>4</code>个消费者节点，构成<code>消费者组</code>，一个<code>partition</code>对应一个消费者节点。从理论上说，这套方案是能够保证消息顺序的。<img src="https://cdn.jsdelivr.net/gh/hswsp/IMAGE_HOST/img/642.png" srcset="/img/loading.gif" lazyload alt="Image"></p><p>一切规划得看似“天衣无缝”，我们就这样”顺利“上线了。</p><h2 id="出现意外">3.出现意外</h2><p>该功能上线了一段时间，刚开始还是比较正常的。</p><p>但是，好景不长，很快就收到用户投诉，说在划菜客户端有些订单和菜品一直看不到，无法划菜。</p><p>我定位到了原因，公司在那段时间网络经常不稳定，业务接口时不时报超时，业务请求时不时会连不上数据库。</p><p>这种情况对<code>顺序消息</code>的打击，可以说是<code>毁灭性</code>的。</p><p>为什么这么说？</p><p>假设订单系统发了：”下单“、”支付“、”完成“ 三条消息。<img src="https://cdn.jsdelivr.net/gh/hswsp/IMAGE_HOST/img/643.png" srcset="/img/loading.gif" lazyload alt="Image"></p><p>而”下单“消息由于网络原因我们系统处理失败了，而后面的两条消息的数据是无法入库的，因为只有”下单“消息的数据才是完整的数据，其他类型的消息只会更新状态。</p><p>加上，我们当时没有做<code>失败重试机制</code>，使得这个问题被放大了。问题变成：一旦”下单“消息的数据入库失败，用户就永远看不到这个订单和菜品了。</p><p>那么这个紧急的问题要如何解决呢？</p><h2 id="解决过程">4.解决过程</h2><p>最开始我们的想法是：在消费者处理消息时，如果处理失败了，立马重试3-5次。但如果有些请求要第6次才能成功怎么办？不可能一直重试呀，这种同步重试机制，会阻塞其他商户订单消息的读取。</p><p>显然用上面的这种<code>同步重试机制</code>在出现异常的情况，会严重影响消息消费者的消费速度，降低它的吞吐量。</p><p>如此看来，我们不得不用<code>异步重试机制</code>了。</p><p>如果用异步重试机制，处理失败的消息就得保存到<code>重试表</code>下来。</p><p>但有个新问题立马出现：<strong>只存一条消息如何保证顺序？</strong></p><p>存一条消息的确无法保证顺序，假如：”下单“消息失败了，还没来得及异步重试。此时，”支付“消息被消费了，它肯定是不能被正常消费的。</p><p>此时，”支付“消息该一直等着，每隔一段时间判断一次，它前面的消息都有没有被消费?</p><p>如果真的这么做，会出现两个问题：</p><ol type="1"><li>”支付“消息前面只有”下单“消息，这种情况比较简单。但如果某种类型的消息，前面有N多种消息，需要判断多少次呀，这种判断跟订单系统的耦合性太强了，相当于要把他们系统的逻辑搬一部分到我们系统。</li><li>影响消费者的消费速度</li></ol><p>这时有种更简单的方案浮出水面：消费者在处理消息时，先判断该<code>订单号</code>在<code>重试表</code>有没有数据，如果有则直接把当前消息保存到<code>重试表</code>。如果没有，则进行业务处理，如果出现异常，把该消息保存到<code>重试表</code>。</p><p>后来我们用<code>elastic-job</code>建立了<code>失败重试机制</code>，如果重试了<code>7</code>次后还是失败，则将该消息的状态标记为<code>失败</code>，发邮件通知开发人员。</p><p>终于由于网络不稳定，导致用户在划菜客户端有些订单和菜品一直看不到的问题被解决了。现在商户顶多偶尔延迟看到菜品，比一直看不菜品好太多。</p><h1 id="消息积压">消息积压</h1><p>随着销售团队的市场推广，我们系统的商户越来越多。随之而来的是消息的数量越来越大，导致消费者处理不过来，经常出现消息积压的情况。对商户的影响非常直观，划菜客户端上的订单和菜品可能半个小时后才能看到。一两分钟还能忍，半个消息的延迟，对有些暴脾气的商户哪里忍得了，马上投诉过来了。我们那段时间经常接到商户投诉说订单和菜品有延迟。</p><p>虽说，加<code>服务器节点</code>就能解决问题，但是按照公司为了省钱的惯例，要先做系统优化，所以我们开始了<code>消息积压</code>问题解决之旅。</p><h2 id="消息体过大">1. 消息体过大</h2><p>虽说<code>kafka</code>号称支持<code>百万级的TPS</code>，但从<code>producer</code>发送消息到<code>broker</code>需要一次网络<code>IO</code>，<code>broker</code>写数据到磁盘需要一次磁盘<code>IO</code>（写操作），<code>consumer</code>从<code>broker</code>获取消息先经过一次磁盘<code>IO</code>（读操作），再经过一次网络<code>IO</code>。<img src="https://cdn.jsdelivr.net/gh/hswsp/IMAGE_HOST/img/644.png" srcset="/img/loading.gif" lazyload alt="Image"></p><p>一次简单的消息从生产到消费过程，需要经过<code>2次网络IO</code>和<code>2次磁盘IO</code>。如果消息体过大，势必会增加IO的耗时，进而影响kafka生产和消费的速度。消费者速度太慢的结果，就会出现消息积压情况。</p><p>除了上面的问题之外，<code>消息体过大</code>，还会浪费服务器的磁盘空间，稍不注意，可能会出现磁盘空间不足的情况。</p><p>此时，我们已经到了需要优化消息体过大问题的时候。</p><p><strong>如何优化呢？</strong></p><p>我们重新梳理了一下业务，没有必要知道订单的<code>中间状态</code>，只需知道一个<code>最终状态</code>就可以了。</p><p>如此甚好，我们就可以这样设计了：</p><ol type="1"><li>订单系统发送的消息体只用包含：id和状态等关键信息。</li><li>后厨显示系统消费消息后，通过id调用订单系统的订单详情查询接口获取数据。</li><li>后厨显示系统判断数据库中是否有该订单的数据，如果没有则入库，有则更新。</li></ol><figure><img src="https://cdn.jsdelivr.net/gh/hswsp/IMAGE_HOST/img/645.png" srcset="/img/loading.gif" lazyload alt="Image"><figcaption aria-hidden="true">Image</figcaption></figure><p>果然这样调整之后，消息积压问题很长一段时间都没再出现。</p><h3 id="section"></h3><h2 id="路由规则不合理">2. 路由规则不合理</h2><p>还真别高兴的太早，有天中午又有商户投诉说订单和菜品有延迟。我们一查kafka的topic竟然又出现了消息积压。</p><p>但这次有点诡异，不是所有<code>partition</code>上的消息都有积压，而是只有一个。</p><figure><img src="https://cdn.jsdelivr.net/gh/hswsp/IMAGE_HOST/img/646.png" srcset="/img/loading.gif" lazyload alt="Image"><figcaption aria-hidden="true">Image</figcaption></figure><p>刚开始，我以为是消费那个<code>partition</code>消息的节点出了什么问题导致的。但是经过排查，没有发现任何异常。</p><p>这就奇怪了，到底哪里有问题呢？</p><p>后来，我查日志和数据库发现，有几个商户的订单量特别大，刚好这几个商户被分到同一个<code>partition</code>，使得该<code>partition</code>的消息量比其他<code>partition</code>要多很多。</p><p>这时我们才意识到，发消息时按<code>商户编号</code>路由<code>partition</code>的规则不合理，可能会导致有些<code>partition</code>消息太多，消费者处理不过来，而有些<code>partition</code>却因为消息太少，消费者出现空闲的情况。</p><p>为了避免出现这种分配不均匀的情况，我们需要对发消息的路由规则做一下调整。</p><p>我们思考了一下，用订单号做路由相对更均匀，不会出现单个订单发消息次数特别多的情况。除非是遇到某个人一直加菜的情况，但是加菜是需要花钱的，所以其实同一个订单的消息数量并不多。</p><p>调整后按<code>订单号</code>路由到不同的<code>partition</code>，同一个订单号的消息，每次到发到同一个<code>partition</code>。</p><figure><img src="https://cdn.jsdelivr.net/gh/hswsp/IMAGE_HOST/img/647.png" srcset="/img/loading.gif" lazyload alt="Image"><figcaption aria-hidden="true">Image</figcaption></figure><p>调整后，消息积压的问题又有很长一段时间都没有再出现。我们的商户数量在这段时间，增长的非常快，越来越多了。</p><h2 id="批量操作引起的连锁反应">3. 批量操作引起的连锁反应</h2><p>在高并发的场景中，消息积压问题，可以说如影随形，真的没办法从根本上解决。表面上看，已经解决了，但后面不知道什么时候，就会冒出一次，比如这次：</p><p>有天下午，产品过来说：有几个商户投诉过来了，他们说菜品有延迟，快查一下原因。</p><p>这次问题出现得有点奇怪。</p><p>为什么这么说？</p><p>首先这个时间点就有点奇怪，平常出问题，不都是中午或者晚上用餐高峰期吗？怎么这次问题出现在下午？</p><p>根据以往积累的经验，我直接看了<code>kafka</code>的<code>topic</code>的数据，果然上面消息有积压，但这次每个<code>partition</code>都积压了<code>十几万</code>的消息没有消费，比以往加压的消息数量增加了<code>几百倍</code>。这次消息积压得极不寻常。</p><p>我赶紧查服务监控看看消费者挂了没，还好没挂。又查服务日志没有发现异常。这时我有点迷茫，碰运气问了问订单组下午发生了什么事情没？他们说下午有个促销活动，跑了一个JOB批量更新过有些商户的订单信息。</p><p>这时，我一下子如梦初醒，是他们在JOB中批量发消息导致的问题。怎么没有通知我们呢？实在太坑了。</p><p>虽说知道问题的原因了，倒是眼前积压的这<code>十几万</code>的消息该如何处理呢？</p><p>此时，如果直接调大<code>partition</code>数量是不行的，历史消息已经存储到4个固定的<code>partition</code>，只有新增的消息才会到新的<code>partition</code>。我们重点需要处理的是已有的partition。</p><p>直接加服务节点也不行，因为<code>kafka</code>允许同组的多个<code>partition</code>被一个<code>consumer</code>消费，但不允许一个<code>partition</code>被同组的多个<code>consumer</code>消费，可能会造成资源浪费。</p><p>看来只有用多线程处理了。</p><p>为了紧急解决问题，我改成了用<code>线程池</code>处理消息，核心线程和最大线程数都配置成了<code>50</code>。</p><p>调整之后，果然，消息积压数量不断减少。</p><p>但此时有个更严重的问题出现：我收到了报警邮件，有两个订单系统的节点down机了。</p><p>不久，订单组的同事过来找我说，我们系统调用他们订单查询接口的并发量突增，超过了预计的好几倍，导致有2个服务节点挂了。他们把查询功能单独整成了一个服务，部署了6个节点，挂了2个节点，再不处理，另外4个节点也会挂。订单服务可以说是公司最核心的服务，它挂了公司损失会很大，情况万分紧急。</p><p>为了解决这个问题，只能先把线程数调小。</p><p>幸好，线程数是可以通过<code>zookeeper</code>动态调整的，我把核心线程数调成了<code>8</code>个，核心线程数改成了<code>10</code>个。</p><p>后面，运维把订单服务挂的2个节点重启后恢复正常了，以防万一，再多加了2个节点。为了确保订单服务不会出现问题，就保持目前的消费速度，后厨显示系统的消息积压问题，1小时候后也恢复正常了。<img src="https://cdn.jsdelivr.net/gh/hswsp/IMAGE_HOST/img/648.png" srcset="/img/loading.gif" lazyload alt="Image"></p><p>后来，我们开了一次复盘会，得出的结论是：</p><ol type="1"><li>订单系统的批量操作一定提前通知下游系统团队。</li><li>下游系统团队多线程调用订单查询接口一定要做压测。</li><li>这次给订单查询服务敲响了警钟，它作为公司的核心服务，应对高并发场景做的不够好，需要做优化。</li><li>对消息积压情况加监控。</li></ol><p>顺便说一下，对于要求严格保证消息顺序的场景，可以将线程池改成多个队列，每个队列用单线程处理。</p><h2 id="表过大">4. 表过大</h2><p>为了防止后面再次出现消息积压问题，消费者后面就一直用多线程处理消息。</p><p>但有天中午我们还是收到很多报警邮件，提醒我们kafka的topic消息有积压。我们正在查原因，此时产品跑过来说：又有商户投诉说菜品有延迟，赶紧看看。这次她看起来有些不耐烦，确实优化了很多次，还是出现了同样的问题。</p><p>在外行看来：<strong>为什么同一个问题一直解决不了？</strong></p><p><strong>其实技术心里的苦他们是不知道的。</strong></p><p>表面上问题的症状是一样的，都是出现了菜品延迟，他们知道的是因为消息积压导致的。但是他们不知道深层次的原因，导致消息积压的原因其实有很多种。这也许是使用消息中间件的通病吧。</p><p>我沉默不语，只能硬着头皮定位原因了。</p><p>后来我查日志发现消费者消费一条消息的耗时长达<code>2秒</code>。以前是<code>500毫秒</code>，现在怎么会变成<code>2秒</code>呢？</p><p>奇怪了，消费者的代码也没有做大的调整，为什么会出现这种情况呢？</p><p>查了一下线上菜品表，单表数据量竟然到了<code>几千万</code>，其他的划菜表也是一样，现在单表保存的数据太多了。</p><p>我们组梳理了一下业务，其实菜品在客户端只展示最近<code>3天</code>的即可。</p><p>这就好办了，我们服务端存着<code>多余的数据</code>，不如把表中多余的数据归档。于是，DBA帮我们把数据做了归档，只保留最近<code>7天</code>的数据。</p><p>如此调整后，消息积压问题被解决了，又恢复了往日的平静。</p><h1 id="主键冲突">主键冲突</h1><p>别高兴得太早了，还有其他的问题，比如：报警邮件经常报出数据库异常：<code>Duplicate entry '6' for key 'PRIMARY'</code>，说主键冲突。</p><p>出现这种问题一般是由于有两个以上相同主键的sql，同时插入数据，第一个插入成功后，第二个插入的时候会报主键冲突。表的主键是唯一的，不允许重复。</p><p>我仔细检查了代码，发现代码逻辑会先根据主键从表中查询订单是否存在，如果存在则更新状态，不存在才插入数据，没得问题。</p><p>这种判断在并发量不大时，是有用的。但是如果在高并发的场景下，两个请求同一时刻都查到订单不存在，一个请求先插入数据，另一个请求再插入数据时就会出现主键冲突的异常。</p><p>解决这个问题最常规的做法是：<code>加锁</code>。</p><p>我刚开始也是这样想的，加数据库悲观锁肯定是不行的，太影响性能。加数据库乐观锁，基于版本号判断，一般用于更新操作，像这种插入操作基本上不会用。</p><p>剩下的只能用分布式锁了，我们系统在用redis，可以加基于redis的分布式锁，锁定订单号。</p><p>但后面仔细思考了一下：</p><ol type="1"><li>加分布式锁也可能会影响消费者的消息处理速度。</li><li>消费者依赖于redis，如果redis出现网络超时，我们的服务就悲剧了。</li></ol><p>所以，我也不打算用分布式锁。</p><p>而是选择使用mysql的<code>INSERT INTO ...ON DUPLICATE KEY UPDATE</code>语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> <span class="hljs-keyword">table</span> (column_list)<br><span class="hljs-keyword">VALUES</span> (value_list)<br><span class="hljs-keyword">ON</span> DUPLICATE KEY <span class="hljs-keyword">UPDATE</span><br>c1 <span class="hljs-operator">=</span> v1, <br>c2 <span class="hljs-operator">=</span> v2,<br>...;<br></code></pre></td></tr></table></figure><p>它会先尝试把数据插入表，如果主键冲突的话那么更新字段。</p><p>把以前的<code>insert</code>语句改造之后，就没再出现过主键冲突问题。</p><h1 id="数据库主从延迟">数据库主从延迟</h1><p>不久之后的某天，又收到商户投诉说下单后，在划菜客户端上看得到订单，但是看到的菜品不全，有时甚至订单和菜品数据都看不到。</p><p>这个问题跟以往的都不一样，根据以往的经验先看<code>kafka</code>的<code>topic</code>中消息有没有积压，但这次并没有积压。</p><p>再查了服务日志，发现订单系统接口返回的数据有些为空，有些只返回了订单数据，没返回菜品数据。</p><p>这就非常奇怪了，我直接过去找订单组的同事。他们仔细排查服务，没有发现问题。这时我们不约而同的想到，会不会是数据库出问题了，一起去找<code>DBA</code>。果然，<code>DBA</code>发现数据库的主库同步数据到从库，由于网络原因偶尔有延迟，有时延迟有<code>3秒</code>。</p><p>如果我们的业务流程从发消息到消费消息耗时小于<code>3秒</code>，调用订单详情查询接口时，可能会查不到数据，或者查到的不是最新的数据。</p><p>这个问题非常严重，会导致直接我们的数据错误。</p><p>为了解决这个问题，我们也加了<code>重试机制</code>。调用接口查询数据时，如果返回数据为空，或者只返回了订单没有菜品，则加入<code>重试表</code>。</p><p>调整后，商户投诉的问题被解决了。</p><h1 id="重复消费">重复消费</h1><p><code>kafka</code>消费消息时支持三种模式：</p><ul><li>at most once模式 最多一次。保证每一条消息commit成功之后，再进行消费处理。消息可能会丢失，但不会重复。</li><li>at least once模式 至少一次。保证每一条消息处理成功之后，再进行commit。消息不会丢失，但可能会重复。</li><li>exactly once模式 精确传递一次。将offset作为唯一id与消息同时处理，并且保证处理的原子性。消息只会处理一次，不丢失也不会重复。但这种方式很难做到。</li></ul><p><code>kafka</code>默认的模式是<code>at least once</code>，但这种模式可能会产生重复消费的问题，所以我们的业务逻辑必须做幂等设计。</p><p>而我们的业务场景保存数据时使用了<code>INSERT INTO ...ON DUPLICATE KEY UPDATE</code>语法，不存在时插入，存在时更新，是天然支持幂等性的。</p><h1 id="多环境消费问题">多环境消费问题</h1><p>我们当时线上环境分为：<code>pre</code>(预发布环境) 和 <code>prod</code>(生产环境)，两个环境共用同一个数据库，并且共用同一个kafka集群。</p><p>需要注意的是，在配置<code>kafka</code>的<code>topic</code>的时候，要加前缀用于区分不同环境。pre环境的以pre_开头，比如：pre_order，生产环境以prod_开头，比如：prod_order，防止消息在不同环境中串了。</p><p>但有次运维在<code>pre</code>环境切换节点，配置<code>topic</code>的时候，配错了，配成了<code>prod</code>的<code>topic</code>。刚好那天，我们有新功能上<code>pre</code>环境。结果悲剧了，<code>prod</code>的有些消息被<code>pre</code>环境的<code>consumer</code>消费了，而由于消息体做了调整，导致<code>pre</code>环境的<code>consumer</code>处理消息一直失败。</p><p>其结果是生产环境丢了部分消息。不过还好，最后生产环境消费者通过重置<code>offset</code>，重新读取了那一部分消息解决了问题，没有造成太大损失。</p><h1 id="后记">后记</h1><p>除了上述问题之外，我还遇到过：</p><ul><li><code>kafka</code>的<code>consumer</code>使用自动确认机制，导致<code>cpu使用率100%</code>。</li><li><code>kafka</code>集群中的一个<code>broker</code>节点挂了，重启后又一直挂。</li></ul><p>这两个问题说起来有些复杂，我就不一一列举了，有兴趣的朋友可以关注我的公众号，加我的微信找我私聊。</p><p>非常感谢那两年使用消息中间件<code>kafka</code>的经历，虽说遇到过挺多问题，踩了很多坑，走了很多弯路，但是实打实的让我积累了很多宝贵的经验，快速成长了。</p><p>其实<code>kafka</code>是一个非常优秀的消息中间件，我所遇到的绝大多数问题，都并非<code>kafka</code>自身的问题（除了cpu使用率100%是它的一个bug导致的之外）。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Infrastructure/" class="category-chain-item">Infrastructure</a> <span>></span> <a href="/categories/Infrastructure/MQ/" class="category-chain-item">MQ</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/MQ/">#MQ</a></div></div><div class="license-box my-3"><div class="license-title"><div>Kafka两年踩过的一些非比寻常的坑</div><div>https://www.blog.spphoto.top/2022/11/06/kafka-practice/</div></div><div class="license-meta"><div class="license-meta-item"><div>Author</div><div>Starry</div></div><div class="license-meta-item license-meta-date"><div>Posted on</div><div>November 6, 2022</div></div><div class="license-meta-item"><div>Licensed under</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - Attribution"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2022/11/07/Troubleshoot-High-CPU-Usage/" title="JVM CPU 使用率飙高问题的排查过程"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">JVM CPU 使用率飙高问题的排查过程</span> <span class="visible-mobile">Previous</span></a></article><article class="post-next col-6"><a href="/2022/11/06/thread-group/" title="Java ThreadGroup"><span class="hidden-mobile">Java ThreadGroup</span> <span class="visible-mobile">Next</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div class="disqus" style="width:100%"><div id="disqus_thread"></div><script type="text/javascript">var disqus_config=function(){this.page.url="https://www.blog.spphoto.top/2022/11/06/kafka-practice/",this.page.identifier="/2022/11/06/kafka-practice/"};Fluid.utils.loadComments("#disqus_thread",(function(){var t=document,e=t.createElement("script");e.src="//fluid.disqus.com/embed.js",e.setAttribute("data-timestamp",new Date),(t.head||t.body).appendChild(e)}))</script><noscript>Please enable JavaScript to view the comments</noscript></div></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><script>Fluid.utils.createScript("https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js",(function(){mermaid.initialize({theme:"default"})}))</script><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">Search</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">Keyword</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="beian"><span><a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">京ICP证123456号 </a></span><span><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=12345678" rel="nofollow noopener" class="beian-police" target="_blank"><span style="visibility:hidden;width:0">|</span> <img src="/img/police_beian.png" srcset="/img/loading.gif" lazyload alt="police-icon"> <span>京公网安备12345678号</span></a></span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var o=jQuery("#board-ctn").offset().top;window.tocbot.init({tocSelector:"#toc-body",contentSelector:".markdown-body",headingSelector:CONFIG.toc.headingSelector||"h1,h2,h3,h4,h5,h6",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",collapseDepth:CONFIG.toc.collapseDepth||0,scrollSmooth:!0,headingsOffset:-o}),t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var o=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),n=[];for(var i of o)n.push(".markdown-body > "+i.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(n.join(", "))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/qipao.js"></script><script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/caidai.js"></script><script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/jingtaisidai.js"></script><script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiantiao.js"></script><script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiaoxuehua.js"></script><script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiaoxingxing.js"></script><script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/timeDate.js"></script><script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/love.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">Blog works best with JavaScript enabled</div></noscript><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body></html>